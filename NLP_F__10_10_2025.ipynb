{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXb0cBSqRUG+fSI1hiU4km",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivasaireddy-prog/NLP/blob/main/NLP_F__10_10_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeRh5QHOyJ8m",
        "outputId": "57ee02c7-ae38-4acb-f3d9-d65d402bec5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Task 1: LDA Topic Tuning ===\n",
            "\n",
            "--- LDA with 3 topics ---\n",
            "Topic #1: team, venture, startup, scale, platform\n",
            "Topic #2: economic, tightens, quarter, policy, predict\n",
            "Topic #3: war, new, services, released, smartphones\n",
            "\n",
            "--- LDA with 5 topics ---\n",
            "Topic #1: team, new, scored, signed, striker\n",
            "Topic #2: war, soldiers, reported, lines, impact\n",
            "Topic #3: new, tech, services, released, innovation\n",
            "Topic #4: war, venture, scale, startup, platform\n",
            "Topic #5: economic, tightens, quarter, policy, predict\n",
            "\n",
            "--- LDA with 8 topics ---\n",
            "Topic #1: war, new, economic, infrastructure, reported\n",
            "Topic #2: stock, central, lending, influence, market\n",
            "Topic #3: war, ceasefire, areas, aid, negotiate\n",
            "Topic #4: war, new, economic, infrastructure, reported\n",
            "Topic #5: new, economic, services, released, tech\n",
            "Topic #6: team, trick, scored, signed, striker\n",
            "Topic #7: war, new, economic, infrastructure, reported\n",
            "Topic #8: venture, startup, scale, learning, platform\n",
            "\n",
            "Separation score for k=3: 1.0000\n",
            "Separation score for k=5: 0.9778\n",
            "Separation score for k=8: 0.8542\n",
            "\n",
            "Recommended k by separation score: 3\n",
            "Explanation: separation score = 1 - mean(pairwise Jaccard of top-word sets). Higher => less overlap => clearer separation.\n",
            "\n",
            "=== Task 2: WordNet Hypernyms & Hyponyms ===\n",
            "\n",
            "Chosen synset: Synset('war.n.01') - the waging of armed conflict against an enemy\n",
            "\n",
            "Hypernyms (broader terms):\n",
            "action, military action\n",
            "\n",
            "Hyponyms (narrower terms):\n",
            "BW, IW, bioattack, biologic attack, biological attack, biological warfare, chemical operations, chemical warfare, civil war, hot war, information warfare, international jihad, jehad, jihad, limited war, psychological warfare, war of nerves, world war\n",
            "\n",
            "Discussion: Hyponyms like 'civil war', 'world war i/ii', 'guerrilla warfare', 'cold war' can be used to build fine-grained subtopics:\n",
            " - 'world war i/ii' -> historical global conflicts subtopic\n",
            " - 'civil war' -> internal conflicts subtopic\n",
            " - 'guerrilla warfare' -> tactics/asymmetric warfare subtopic\n",
            "\n",
            "=== Task 3: Jaccard Similarity (Unigram vs Bigram) ===\n",
            "\n",
            "Document A: A major military war broke out in the region, leading to peace talks and international sanctions.\n",
            "Document B: Diplomats met to negotiate a ceasefire and humanitarian aid to war-affected areas.\n",
            "Document C (contrast): Tech companies released new smartphones and cloud services, boosting innovation in AI and data.\n",
            "\n",
            "Unigram-based Jaccard similarities:\n",
            "Jaccard(A, B) = 0.1667 (|I|=4, |U|=24)\n",
            "Jaccard(A, C) = 0.0741 (|I|=2, |U|=27)\n",
            "\n",
            "Bigram-based Jaccard similarities:\n",
            "Jaccard(A, B) = 0.0000 (|I|=0, |U|=27)\n",
            "Jaccard(A, C) = 0.0000 (|I|=0, |U|=28)\n",
            "\n",
            "Interpretation:\n",
            "- Unigram Jaccard measures shared words; can be high if documents share vocabulary.\n",
            "- Bigram Jaccard measures shared contiguous 2-word phrases; it captures phrase-level similarity and ordering.\n",
            "- Bigram similarity often gives a tighter sense of relatedness when phrase structure matters; unigram is coarser.\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "DATA_PATH = \"cord19_abstracts.csv\"   # CSV expected to have column 'abstract'\n",
        "TEXT_COLUMN = \"abstract\"\n",
        "MIN_ENTITY_LEN = 2                   # minimum length (chars) for entity tokens\n",
        "TOP_N_DISPLAY = 30                   # for plotting: number of top nodes to show\n",
        "TOP_K_CENTRAL = 10                   # top-K central nodes to print\n",
        "SPACY_MODEL = \"en_core_web_sm\"       # spaCy model\n",
        "OUTPUT_GRAPH_GPKL = \"cooccurrence_graph.gpickle\"\n",
        "OUTPUT_CENTRALITY_CSV = \"centrality_all_nodes.csv\"\n",
        "# -----------------------------------------------\n",
        "\n",
        "# Load spaCy model (exit with helpful message if missing)\n",
        "try:\n",
        "    nlp = spacy.load(SPACY_MODEL)\n",
        "except Exception as e:\n",
        "    raise SystemExit(\n",
        "        f\"spaCy model '{SPACY_MODEL}' is not available. Install with:\\n\"\n",
        "        f\"  pip install spacy\\n\"\n",
        "        f\"  python -m spacy download {SPACY_MODEL}\\n\\nOriginal error: {e}\"\n",
        "    )\n",
        "\n",
        "def create_synthetic_sample(n=500, seed=42):\n",
        "    \"\"\"Create a small synthetic abstracts dataset if real file is not present.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    positives = [\n",
        "        \"Deep learning improves sentiment analysis by learning hierarchical features.\",\n",
        "        \"Graph neural networks show promising results for molecular property prediction.\",\n",
        "        \"Transfer learning increases performance in low-resource clinical NLP tasks.\",\n",
        "        \"Transformer architectures achieve state-of-the-art on protein structure prediction.\",\n",
        "        \"Data augmentation improves generalization for small datasets.\"\n",
        "    ]\n",
        "    negatives = [\n",
        "        \"Method fails on out-of-distribution examples and needs further study.\",\n",
        "        \"The dataset is noisy and labels are inconsistent across annotators.\",\n",
        "        \"Model performance drops sharply in adversarial settings.\",\n",
        "        \"This approach is not suitable for real-time deployment due to latency.\",\n",
        "        \"Results are inconclusive and require additional experiments.\"\n",
        "    ]\n",
        "    rows = []\n",
        "    for i in range(n):\n",
        "        text = np.random.choice(positives + negatives)\n",
        "        rows.append({\"abstract\": text})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def load_data(path=DATA_PATH, text_col=TEXT_COLUMN):\n",
        "    \"\"\"Load dataset or fallback to synthetic sample (with message).\"\"\"\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        if text_col not in df.columns:\n",
        "            raise ValueError(f\"File found but missing required column '{text_col}'. Columns: {list(df.columns)}\")\n",
        "        df = df[[text_col]].dropna().rename(columns={text_col: \"text\"})\n",
        "        print(f\"Loaded {len(df)} documents from {path}\")\n",
        "    else:\n",
        "        print(f\"WARNING: Data file '{path}' not found. Creating a synthetic sample for demo.\")\n",
        "        df = create_synthetic_sample(n=500)\n",
        "        df = df.rename(columns={\"abstract\": \"text\"})\n",
        "        print(f\"Synthetic dataset created with {len(df)} documents.\")\n",
        "    return df\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Basic text cleaning.\"\"\"\n",
        "    text = str(text).replace(\"\\n\", \" \").strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "def extract_entities_from_sentence(sent_span, min_len=MIN_ENTITY_LEN):\n",
        "    \"\"\"\n",
        "    Extract entity candidates from a sentence:\n",
        "    - prefer noun_chunks (multi-word noun phrases)\n",
        "    - also include single nouns/proper nouns not in noun_chunks\n",
        "    Return a list of normalized entity strings.\n",
        "    \"\"\"\n",
        "    ents = []\n",
        "    # noun_chunks sometimes include determiners/pronouns; use lemma form and lowercase\n",
        "    for nc in sent_span.noun_chunks:\n",
        "        s = nc.lemma_.lower().strip()\n",
        "        s = re.sub(r\"[^a-z0-9\\s\\-]\", \"\", s)\n",
        "        if len(s) >= min_len and not s.isdigit():\n",
        "            if s not in ents:\n",
        "                ents.append(s)\n",
        "    for token in sent_span:\n",
        "        if token.pos_ in (\"NOUN\", \"PROPN\") and token.lemma_:\n",
        "            s = token.lemma_.lower().strip()\n",
        "            s = re.sub(r\"[^a-z0-9\\s\\-]\", \"\", s)\n",
        "            if len(s) >= min_len and s not in ents:\n",
        "                ents.append(s)\n",
        "    return ents\n",
        "\n",
        "def extract_svo_from_sentence(sent_span):\n",
        "    \"\"\"\n",
        "    Lightweight SVO extraction:\n",
        "    For each verb token, look for nominal subject and (direct) object children.\n",
        "    Returns list of (subj_lemma, verb_lemma, obj_lemma)\n",
        "    \"\"\"\n",
        "    svos = []\n",
        "    for token in sent_span:\n",
        "        if token.pos_ == \"VERB\":\n",
        "            subj = None\n",
        "            obj = None\n",
        "            # check children\n",
        "            for child in token.children:\n",
        "                if child.dep_ in (\"nsubj\", \"nsubjpass\") and child.pos_ in (\"NOUN\", \"PROPN\", \"PRON\"):\n",
        "                    subj = child\n",
        "                if child.dep_ in (\"dobj\", \"obj\", \"pobj\") and child.pos_ in (\"NOUN\", \"PROPN\"):\n",
        "                    obj = child\n",
        "            if subj is not None and obj is not None:\n",
        "                svos.append((subj.lemma_.lower(), token.lemma_.lower(), obj.lemma_.lower()))\n",
        "    return svos\n",
        "\n",
        "def build_cooccurrence_graph(texts):\n",
        "    \"\"\"\n",
        "    Build an undirected co-occurrence graph where:\n",
        "      - nodes: entity strings (noun phrases / nouns)\n",
        "      - edges: increment weight when two entities co-occur in the same sentence\n",
        "    Also collects SVO triples encountered.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    cooc_counts = Counter()\n",
        "    node_counts = Counter()\n",
        "    relations = []\n",
        "\n",
        "    # iterate with spaCy pipe for speed\n",
        "    for doc in nlp.pipe(texts, disable=[\"ner\"]):\n",
        "        for sent in doc.sents:\n",
        "            sent_doc = sent.as_doc()  # treat span like doc for convenience\n",
        "            ents = extract_entities_from_sentence(sent_doc)\n",
        "            # update node counts\n",
        "            for e in ents:\n",
        "                node_counts[e] += 1\n",
        "            # update co-occurrence\n",
        "            for i in range(len(ents)):\n",
        "                for j in range(i+1, len(ents)):\n",
        "                    a, b = ents[i], ents[j]\n",
        "                    if a == b:\n",
        "                        continue\n",
        "                    key = tuple(sorted([a, b]))\n",
        "                    cooc_counts[key] += 1\n",
        "            # extract SVOs\n",
        "            svos = extract_svo_from_sentence(sent_doc)\n",
        "            for s, v, o in svos:\n",
        "                relations.append((s, v, o))\n",
        "\n",
        "    # add nodes and node attributes\n",
        "    for node, cnt in node_counts.items():\n",
        "        G.add_node(node, count=cnt)\n",
        "    # add edges with weights\n",
        "    for (a, b), w in cooc_counts.items():\n",
        "        G.add_edge(a, b, weight=w)\n",
        "\n",
        "    print(f\"Built graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
        "    return G, relations\n",
        "\n",
        "def visualize_graph_nx(G, top_n=TOP_N_DISPLAY, figsize=(12, 8), seed=42):\n",
        "    \"\"\"\n",
        "    Visualize a reduced version of the graph (top_n by degree) using NetworkX and matplotlib.\n",
        "    \"\"\"\n",
        "    if G.number_of_nodes() == 0:\n",
        "        print(\"Graph is empty, nothing to visualize.\")\n",
        "        return\n",
        "\n",
        "    # choose top-n nodes by weighted degree\n",
        "    deg = dict(G.degree(weight='weight'))\n",
        "    sorted_nodes = sorted(deg.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_nodes = set([n for n, _ in sorted_nodes[:top_n]])\n",
        "    H = G.subgraph(top_nodes).copy()\n",
        "\n",
        "    pos = nx.spring_layout(H, seed=seed, k=0.5)\n",
        "    edge_weights = [max(0.5, H[u][v].get('weight', 1)) for u, v in H.edges()]\n",
        "    node_sizes = [300 + 50 * H.nodes[n].get('count', 0) for n in H.nodes()]\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    nx.draw_networkx_nodes(H, pos, node_size=node_sizes, alpha=0.9)\n",
        "    nx.draw_networkx_edges(H, pos, width=[math.log(w+1) for w in edge_weights], alpha=0.6)\n",
        "    nx.draw_networkx_labels(H, pos, font_size=9)\n",
        "    plt.title(f\"Co-occurrence Graph (Top {len(H.nodes())} nodes shown)\")\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def compute_and_save_centralities(G, out_csv=OUTPUT_CENTRALITY_CSV, top_k=TOP_K_CENTRAL):\n",
        "    \"\"\"\n",
        "    Compute degree centrality, betweenness, pagerank and eigenvector centrality.\n",
        "    Return top-k nodes by aggregated rank.\n",
        "    \"\"\"\n",
        "    if G.number_of_nodes() == 0:\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    deg_cent = nx.degree_centrality(G)\n",
        "    bet_cent = nx.betweenness_centrality(G, weight='weight', normalized=True)\n",
        "    try:\n",
        "        pr = nx.pagerank(G, weight='weight')\n",
        "    except Exception:\n",
        "        pr = {n: 0.0 for n in G.nodes()}\n",
        "    try:\n",
        "        eig = nx.eigenvector_centrality_numpy(G, weight='weight')\n",
        "    except Exception:\n",
        "        eig = {n: 0.0 for n in G.nodes()}\n",
        "\n",
        "    rows = []\n",
        "    for n in G.nodes():\n",
        "        rows.append({\n",
        "            \"node\": n,\n",
        "            \"count\": G.nodes[n].get('count', 0),\n",
        "            \"degree_centrality\": deg_cent.get(n, 0),\n",
        "            \"betweenness_centrality\": bet_cent.get(n, 0),\n",
        "            \"pagerank\": pr.get(n, 0),\n",
        "            \"eigenvector\": eig.get(n, 0)\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    # rank and aggregate\n",
        "    for col in [\"degree_centrality\", \"betweenness_centrality\", \"pagerank\", \"eigenvector\"]:\n",
        "        df[col + \"_rank\"] = df[col].rank(ascending=False, method=\"min\")\n",
        "    rank_cols = [c + \"_rank\" for c in [\"degree_centrality\", \"betweenness_centrality\", \"pagerank\", \"eigenvector\"]]\n",
        "    df[\"avg_rank\"] = df[rank_cols].mean(axis=1)\n",
        "    df_sorted = df.sort_values(\"avg_rank\").reset_index(drop=True)\n",
        "\n",
        "    df_sorted.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved centrality table to: {out_csv}\")\n",
        "    return df_sorted.head(top_k), df_sorted\n",
        "\n",
        "def save_graph_robust(G, path=OUTPUT_GRAPH_GPKL):\n",
        "    \"\"\"\n",
        "    Save graph robustly:\n",
        "    - prefer networkx native write_gpickle if available\n",
        "    - otherwise fallback to pickle.dump\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Some networkx versions have write_gpickle at top-level\n",
        "        write_fn = getattr(nx, \"write_gpickle\", None)\n",
        "        if callable(write_fn):\n",
        "            write_fn(G, path)\n",
        "            print(f\"Graph saved via networkx.write_gpickle to: {path}\")\n",
        "            return\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # fallback using pickle\n",
        "    try:\n",
        "        with open(path, \"wb\") as f:\n",
        "            pickle.dump(G, f)\n",
        "        print(f\"Graph saved via pickle to: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save graph to {path}: {e}\")\n",
        "\n",
        "# ------------------- MAIN -------------------\n",
        "def main():\n",
        "    df = load_data()\n",
        "    df[\"text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "    texts = df[\"text\"].tolist()\n",
        "    G, relations = build_cooccurrence_graph(texts)\n",
        "\n",
        "    # Show a few SVO relations (if any)\n",
        "    if relations:\n",
        "        print(\"\\nSample SVO triples (first 20):\")\n",
        "        for t in relations[:20]:\n",
        "            print(\" -\", t)\n",
        "    else:\n",
        "        print(\"\\nNo SVO triples were extracted.\")\n",
        "\n",
        "    # Visualize graph with NetworkX\n",
        "    visualize_graph_nx(G, top_n=TOP_N_DISPLAY)\n",
        "\n",
        "    # Compute centralities and show top-K\n",
        "    top_df, all_df = compute_and_save_centralities(G, out_csv=OUTPUT_CENTRALITY_CSV, top_k=TOP_K_CENTRAL)\n",
        "    if not top_df.empty:\n",
        "        print(\"\\nTop central nodes (aggregated ranks):\")\n",
        "        display_cols = [\"node\", \"count\", \"degree_centrality\", \"betweenness_centrality\", \"pagerank\", \"eigenvector\"]\n",
        "        print(top_df[display_cols].to_string(index=False))\n",
        "    else:\n",
        "        print(\"No centrality results (empty graph).\")\n",
        "\n",
        "    # Save graph robustly\n",
        "    save_graph_robust(G, OUTPUT_GRAPH_GPKL)\n",
        "\n",
        "    # Short analysis scaffold\n",
        "    print(\"\\n--- Short analysis ---\")\n",
        "    print(\"1) High-centrality nodes identify core topics/methods (e.g., 'deep learning', 'model').\")\n",
        "    print(\"2) Betweenness highlights bridging concepts connecting subfields (useful for interdisciplinary search).\")\n",
        "    print(\"3) SVO triples provide candidate directed relations (subject -> verb -> object) to populate a typed KG.\")\n",
        "    print(\"4) Researchers can follow neighborhoods of a node to discover related methods, datasets, or findings.\")\n",
        "    print(\"5) For detecting emerging topics, build temporal graphs (year or month windows) and monitor increasing node/edge weights.\")\n",
        "\n",
        "if __name__ == \"__main__\":"
      ]
    }
  ]
}