{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKtiBy4fa9g6IFCnkhC9UO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivasaireddy-prog/NLP/blob/main/NLP_F__31_10_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CeRh5QHOyJ8m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import UnigramTagger, BigramTagger\n",
        "from nltk.probability import LidstoneProbDist\n",
        "from nltk.tag import hmm as nltk_hmm\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Try to import gensim; if missing we'll fallback\n",
        "try:\n",
        "    import gensim\n",
        "    from gensim.models import Word2Vec\n",
        "    GENSIM_AVAILABLE = True\n",
        "except Exception:\n",
        "    GENSIM_AVAILABLE = False\n",
        "\n",
        "# download minimal NLTK resources used (brown). We will not rely on punkt-based tokenizers.\n",
        "nltk.download('brown', quiet=True)\n",
        "nltk.download('universal_tagset', quiet=True)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# ----------------- Lightweight tokenizers (regex-based) -----------------\n",
        "_sent_split_re = re.compile(r'(?<=[.!?])\\s+')\n",
        "_word_token_re = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?|[^\\sA-Za-z0-9]\")\n",
        "\n",
        "def split_sentences(text):\n",
        "    \"\"\"Split text into sentences using a simple regex (avoids NLTK punkt).\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    # Split on punctuation followed by whitespace\n",
        "    sents = _sent_split_re.split(text)\n",
        "    # further strip and filter empties\n",
        "    return [s.strip() for s in sents if s.strip()]\n",
        "\n",
        "def tokenize_words(text):\n",
        "    \"\"\"Tokenize words using regex (avoids NLTK word_tokenize dependence).\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    tokens = _word_token_re.findall(text)\n",
        "    return tokens\n",
        "\n",
        "# ----------------------------- Task 1: HMM-based POS Tagging -----------------------------\n",
        "def train_hmm_tagger():\n",
        "    \"\"\"\n",
        "    Train an HMM POS tagger on the Brown corpus (universal tagset).\n",
        "    Returns a tagger object with .tag(list_of_tokens) method (or fallback tagger).\n",
        "    \"\"\"\n",
        "    print(\"Training POS tagger on Brown corpus (universal tagset)...\")\n",
        "    tagged_sents_view = brown.tagged_sents(tagset='universal')\n",
        "    tagged_sents = list(tagged_sents_view)  # convert to list so we can shuffle\n",
        "    random.shuffle(tagged_sents)\n",
        "    split = int(0.9 * len(tagged_sents))\n",
        "    train_sents = tagged_sents[:split]\n",
        "    test_sents = tagged_sents[split:]\n",
        "\n",
        "    try:\n",
        "        estimator = lambda fd, bins: LidstoneProbDist(fd, 0.1, bins)\n",
        "        trainer = nltk_hmm.HiddenMarkovModelTrainer()\n",
        "        hmm_tagger = trainer.train_supervised(train_sents, estimator=estimator)\n",
        "        print(\"HMM tagger trained successfully.\")\n",
        "        # quick approximate evaluation on a small held-out sample\n",
        "        sample_test = test_sents[:200]\n",
        "        test_tokens = [[w for w,t in s] for s in sample_test]\n",
        "        gold = [[t for w,t in s] for s in sample_test]\n",
        "        pred = [[tag for w,tag in hmm_tagger.tag(sent)] for sent in test_tokens]\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for g,p in zip(gold,pred):\n",
        "            for a,b in zip(g,p):\n",
        "                total += 1\n",
        "                if a == b: correct += 1\n",
        "        acc = correct / total if total > 0 else 0.0\n",
        "        print(f\"HMM approx accuracy on brown sample: {acc:.4f}\")\n",
        "        return hmm_tagger\n",
        "    except Exception as e:\n",
        "        print(\"HMM training failed or is not available. Falling back to Unigram+Bigram. Error:\", e)\n",
        "        unigram = UnigramTagger(train_sents)\n",
        "        bigram = BigramTagger(train_sents, backoff=unigram)\n",
        "        print(\"Fallback BigramTagger trained.\")\n",
        "        return bigram\n",
        "\n",
        "# ----------------------------- Helper: political articles -----------------------------\n",
        "def load_or_create_political_articles(folder=\"political_articles\", n=10):\n",
        "    folder_p = Path(folder)\n",
        "    articles = []\n",
        "    if folder_p.exists() and folder_p.is_dir():\n",
        "        files = list(folder_p.glob(\"*.txt\"))\n",
        "        for f in files[:n]:\n",
        "            articles.append(f.read_text(encoding='utf-8'))\n",
        "    if len(articles) >= n:\n",
        "        print(f\"Loaded {len(articles)} articles from {folder}\")\n",
        "        return articles[:n]\n",
        "    print(f\"Folder '{folder}' not found or insufficient files. Creating {n} synthetic political articles for demo.\")\n",
        "    templates = [\n",
        "        \"The government announced new policies about the election and public health. Experts said this move will affect voter turnout and campaign strategies.\",\n",
        "        \"The opposition criticized the new bill, arguing that it undermines democratic institutions and centralizes power in the executive branch.\",\n",
        "        \"International relations are heated after the summit, with trade tariffs discussed and diplomatic channels strained by recent incidents.\",\n",
        "        \"A major candidate promises economic reforms and job creation, while rivals focus on healthcare and education.\",\n",
        "        \"The parliament voted on the controversial measure, with fierce debates, protests outside, and legal challenges expected.\",\n",
        "    ]\n",
        "    synthetic = []\n",
        "    for i in range(n):\n",
        "        sents = [random.choice(templates) for _ in range(4)]\n",
        "        synthetic.append(\" \".join(sents))\n",
        "    return synthetic\n",
        "\n",
        "def tokenize_sentences_and_words(article_text):\n",
        "    \"\"\"Return list-of-lists tokens per sentence using regex tokenizers.\"\"\"\n",
        "    sents = split_sentences(article_text)\n",
        "    tokenized = [tokenize_words(s) for s in sents]\n",
        "    return tokenized\n",
        "\n",
        "def evaluate_tagger_on_articles(tagger, articles):\n",
        "    all_tagged = []\n",
        "    for art in articles:\n",
        "        toks_sents = tokenize_sentences_and_words(art)\n",
        "        for sent in toks_sents:\n",
        "            try:\n",
        "                tagged = tagger.tag(sent)\n",
        "            except Exception:\n",
        "                tagged = [(w, 'NOUN') for w in sent]\n",
        "            all_tagged.append((sent, tagged))\n",
        "    return all_tagged\n",
        "\n",
        "# ----------------------------- Task 2: Embeddings & t-SNE Visualization -----------------------------\n",
        "def prepare_corpus_for_embedding(all_texts):\n",
        "    tokenized = []\n",
        "    for doc in all_texts:\n",
        "        for sent in split_sentences(doc):\n",
        "            toks = [w.lower() for w in tokenize_words(sent) if any(c.isalnum() for c in w)]\n",
        "            if toks:\n",
        "                tokenized.append(toks)\n",
        "    return tokenized\n",
        "\n",
        "def train_word_embeddings(tokenized_sentences, vector_size=100, window=5, min_count=2):\n",
        "    if GENSIM_AVAILABLE:\n",
        "        print(\"Training Word2Vec embeddings with gensim...\")\n",
        "        model = Word2Vec(sentences=tokenized_sentences, vector_size=vector_size,\n",
        "                         window=window, min_count=min_count, workers=2, seed=RANDOM_SEED)\n",
        "        wv = model.wv\n",
        "        word_vectors = {w: wv[w] for w in wv.index_to_key}\n",
        "        print(f\"Trained Word2Vec with vocab size {len(word_vectors)}\")\n",
        "        return word_vectors, 'word2vec'\n",
        "    else:\n",
        "        print(\"gensim not available. Using CountVectorizer + TruncatedSVD as fallback embeddings...\")\n",
        "        docs = [\" \".join(s) for s in tokenized_sentences]\n",
        "        vectorizer = CountVectorizer(min_df=2)\n",
        "        X = vectorizer.fit_transform(docs)\n",
        "        svd = TruncatedSVD(n_components=vector_size, random_state=RANDOM_SEED)\n",
        "        svd.fit(X)\n",
        "        comp = svd.components_.T\n",
        "        vocab = vectorizer.get_feature_names_out()\n",
        "        word_vectors = {w: comp[i] for i, w in enumerate(vocab)}\n",
        "        print(f\"Fallback embeddings for {len(word_vectors)} words.\")\n",
        "        return word_vectors, 'svd'\n",
        "\n",
        "def tsne_visualize_words(word_vectors, category_word_lists, out_png=\"tsne_words.png\"):\n",
        "    plot_words = []\n",
        "    labels = []\n",
        "    for cat, words in category_word_lists.items():\n",
        "        for w in words:\n",
        "            if w in word_vectors:\n",
        "                plot_words.append(w)\n",
        "                labels.append(cat)\n",
        "            elif w.lower() in word_vectors:\n",
        "                plot_words.append(w.lower())\n",
        "                labels.append(cat)\n",
        "    if not plot_words:\n",
        "        print(\"No words from category lists found in embeddings. Skipping t-SNE.\")\n",
        "        return\n",
        "    vectors = np.vstack([word_vectors[w] for w in plot_words])\n",
        "    tsne = TSNE(n_components=2, perplexity=8, learning_rate=200, random_state=RANDOM_SEED, init='pca')\n",
        "    X2 = tsne.fit_transform(vectors)\n",
        "    df = pd.DataFrame({'x': X2[:,0], 'y': X2[:,1], 'word': plot_words, 'category': labels})\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.scatterplot(data=df, x='x', y='y', hue='category', s=100)\n",
        "    for i,row in df.iterrows():\n",
        "        plt.text(row.x+0.5, row.y+0.5, row.word, fontsize=9)\n",
        "    plt.title(\"t-SNE visualization of selected words by category\")\n",
        "    plt.savefig(out_png, bbox_inches='tight', dpi=200)\n",
        "    plt.show()\n",
        "    print(f\"Saved t-SNE plot to {out_png}\")\n",
        "\n",
        "# ----------------------------- Task 3: News Classification Baseline (NB) and CNN (PyTorch) -----------------------------\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab=None, max_len=200):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.max_len = max_len\n",
        "        if vocab is None:\n",
        "            vect = CountVectorizer(min_df=1)\n",
        "            vect.fit(texts)\n",
        "            self.vocab = {w:i+1 for i,w in enumerate(vect.get_feature_names_out())}\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "        self.unk_idx = len(self.vocab) + 1\n",
        "\n",
        "    def text_to_indices(self, text):\n",
        "        toks = [w.lower() for w in tokenize_words(text) if any(c.isalnum() for c in w)]\n",
        "        idxs = [self.vocab.get(w, self.unk_idx) for w in toks]\n",
        "        if len(idxs) >= self.max_len:\n",
        "            return idxs[:self.max_len]\n",
        "        else:\n",
        "            return idxs + [0] * (self.max_len - len(idxs))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = np.array(self.text_to_indices(self.texts[idx]), dtype=np.int64)\n",
        "        y = np.int64(self.labels[idx])\n",
        "        return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "class CNNTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=100, num_classes=3, kernel_sizes=[3,4,5], num_filters=100, padding_idx=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size+2, embedding_dim=embed_dim, padding_idx=padding_idx)\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=k) for k in kernel_sizes])\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        emb = emb.permute(0,2,1)\n",
        "        conv_outs = [torch.relu(conv(emb)) for conv in self.convs]\n",
        "        pooled = [torch.max(co, dim=2)[0] for co in conv_outs]\n",
        "        cat = torch.cat(pooled, dim=1)\n",
        "        out = self.dropout(cat)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "def train_cnn(model, train_loader, val_loader, epochs=6, lr=1e-3, device='cpu'):\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    best_val = 0.0\n",
        "    best_state = None\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for xb,yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = crit(logits, yb)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "        avg_train_loss = total_loss / len(train_loader.dataset)\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        trues = []\n",
        "        with torch.no_grad():\n",
        "            for xb,yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                logits = model(xb)\n",
        "                ps = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                preds.extend(ps.tolist())\n",
        "                trues.extend(yb.cpu().numpy().tolist())\n",
        "        acc = accuracy_score(trues, preds)\n",
        "        if acc > best_val:\n",
        "            best_val = acc\n",
        "            best_state = model.state_dict()\n",
        "        print(f\"Epoch {ep}: TrainLoss={avg_train_loss:.4f}, ValAcc={acc:.4f}\")\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "def evaluate_model_model_on_texts(model, dataset, device='cpu', batch_size=64):\n",
        "    loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            ps = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            preds.extend(ps.tolist())\n",
        "            trues.extend(yb.numpy().tolist())\n",
        "    return trues, preds\n",
        "\n",
        "# ----------------------------- Demo data for classification (3-class) -----------------------------\n",
        "def load_or_create_classification_dataset():\n",
        "    path = Path(\"news_dataset.csv\")\n",
        "    if path.exists():\n",
        "        df = pd.read_csv(path)\n",
        "        if not {'text','label'}.issubset(df.columns):\n",
        "            raise ValueError(\"news_dataset.csv must contain 'text' and 'label' columns\")\n",
        "        return df['text'].tolist(), df['label'].tolist()\n",
        "    print(\"Creating synthetic news dataset for demonstration (politics, sports, business).\")\n",
        "    politics = [\n",
        "        \"The government passed a new bill that affects election reform and public spending.\",\n",
        "        \"Opposition leaders criticized the policy and called for new oversight committees.\",\n",
        "        \"Diplomatic talks focused on trade agreements and national security cooperation.\",\n",
        "    ]\n",
        "    sports = [\n",
        "        \"The team won the championship after a thrilling match in overtime.\",\n",
        "        \"The coach praised the players' defense and tactical discipline.\",\n",
        "        \"Injury time changed the outcome of the game and fans celebrated the victory.\",\n",
        "    ]\n",
        "    business = [\n",
        "        \"The company reported record profits after launching their new product line.\",\n",
        "        \"Investors reacted to the quarterly results by adjusting share prices.\",\n",
        "        \"Mergers and acquisitions in the sector drove market consolidation.\",\n",
        "    ]\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for _ in range(200):\n",
        "        texts.append(\" \".join(random.choices(politics, k=3)))\n",
        "        labels.append(\"politics\")\n",
        "        texts.append(\" \".join(random.choices(sports, k=3)))\n",
        "        labels.append(\"sports\")\n",
        "        texts.append(\" \".join(random.choices(business, k=3)))\n",
        "        labels.append(\"business\")\n",
        "    return texts, labels\n",
        "\n",
        "# ----------------------------- MAIN RUN -----------------------------\n",
        "def main():\n",
        "    print(\"\\n=== TASK 1: HMM-based POS Tagging ===\")\n",
        "    tagger = train_hmm_tagger()\n",
        "    articles = load_or_create_political_articles(n=10)\n",
        "    tagged_examples = evaluate_tagger_on_articles(tagger, articles)\n",
        "    print(\"\\nSample tagged sentences (first 10):\")\n",
        "    for sent, tagged in tagged_examples[:10]:\n",
        "        print(\"SENT:\", \" \".join(sent))\n",
        "        print(\"TAGGED:\", tagged)\n",
        "        print(\"-----\")\n",
        "\n",
        "    surprising = []\n",
        "    for sent, tagged in tagged_examples:\n",
        "        for w,t in tagged:\n",
        "            if t == '.' or t == ',':\n",
        "                surprising.append((sent, tagged))\n",
        "                break\n",
        "        if len(surprising) >= 5:\n",
        "            break\n",
        "    if surprising:\n",
        "        print(\"\\nExamples potentially problematic (heuristic detection):\")\n",
        "        for sent, tagged in surprising[:5]:\n",
        "            print(\"SENT:\", \" \".join(sent))\n",
        "            print(\"TAGGED:\", tagged)\n",
        "            print(\"-----\")\n",
        "\n",
        "    print(\"\\n=== TASK 2: Embedding training and visualization ===\")\n",
        "    cls_texts, cls_labels = load_or_create_classification_dataset()\n",
        "    corpus_texts = cls_texts + articles\n",
        "    tokenized_sentences = prepare_corpus_for_embedding(corpus_texts)\n",
        "    word_vectors, method = train_word_embeddings(tokenized_sentences, vector_size=100)\n",
        "    print(\"Embedding method used:\", method)\n",
        "\n",
        "    politics_words = [\"election\", \"government\", \"policy\", \"vote\", \"minister\", \"parliament\", \"diplomatic\", \"trade\"]\n",
        "    sports_words = [\"championship\", \"coach\", \"match\", \"score\", \"goal\", \"defense\", \"injury\", \"league\"]\n",
        "    business_words = [\"profit\", \"investor\", \"market\", \"merger\", \"share\", \"acquisition\", \"quarterly\", \"revenue\"]\n",
        "\n",
        "    cat_words = {\n",
        "        'politics': politics_words,\n",
        "        'sports': sports_words,\n",
        "        'business': business_words\n",
        "    }\n",
        "    tsne_visualize_words(word_vectors, cat_words, out_png=\"tsne_categories.png\")\n",
        "\n",
        "    print(\"\\n=== TASK 3: News classification: Naive Bayes vs CNN (PyTorch) ===\")\n",
        "    texts, labels = cls_texts, cls_labels\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(labels)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(texts, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
        "\n",
        "    print(\"\\nTraining Multinomial Naive Bayes baseline...\")\n",
        "    tfv = TfidfVectorizer(min_df=2, ngram_range=(1,2))\n",
        "    Xtr_tfidf = tfv.fit_transform(X_train)\n",
        "    Xte_tfidf = tfv.transform(X_test)\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(Xtr_tfidf, y_train)\n",
        "    nb_preds = nb.predict(Xte_tfidf)\n",
        "    print(\"Naive Bayes classification report:\")\n",
        "    print(classification_report(y_test, nb_preds, target_names=le.classes_))\n",
        "    nb_acc = accuracy_score(y_test, nb_preds)\n",
        "    nb_f1 = f1_score(y_test, nb_preds, average='macro')\n",
        "    print(f\"NB Acc={nb_acc:.4f}, Macro-F1={nb_f1:.4f}\")\n",
        "\n",
        "    print(\"\\nPreparing data and vocab for CNN...\")\n",
        "    train_dataset = NewsDataset(X_train, y_train, vocab=None, max_len=200)\n",
        "    test_dataset = NewsDataset(X_test, y_test, vocab=train_dataset.vocab, max_len=200)\n",
        "    vocab_size = len(train_dataset.vocab) + 2\n",
        "    print(\"Vocab size:\", len(train_dataset.vocab))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=128)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(\"Using device:\", device)\n",
        "    cnn_model = CNNTextClassifier(vocab_size=vocab_size, embed_dim=100, num_classes=len(le.classes_))\n",
        "    cnn_model = train_cnn(cnn_model, train_loader, val_loader, epochs=6, lr=1e-3, device=device)\n",
        "\n",
        "    trues, preds = evaluate_model_model_on_texts(cnn_model, test_dataset, device=device)\n",
        "    print(\"\\nCNN classification report:\")\n",
        "    print(classification_report(trues, preds, target_names=le.classes_))\n",
        "    cnn_acc = accuracy_score(trues, preds)\n",
        "    cnn_f1 = f1_score(trues, preds, average='macro')\n",
        "    print(f\"CNN Acc={cnn_acc:.4f}, Macro-F1={cnn_f1:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Comparison ---\")\n",
        "    print(f\"Naive Bayes: Acc={nb_acc:.4f}, Macro-F1={nb_f1:.4f}\")\n",
        "    print(f\"CNN (1D):    Acc={cnn_acc:.4f}, Macro-F1={cnn_f1:.4f}\")\n",
        "    if cnn_f1 > nb_f1:\n",
        "        print(\"CNN outperformed Naive Bayes on this dataset (likely better at capturing local n-gram structure).\")\n",
        "    else:\n",
        "        print(\"Naive Bayes performed comparably or better â€” on small synthetic/simple datasets NB can be very strong.\")\n",
        "\n",
        "    print(\"\\nSample predictions (CNN):\")\n",
        "    for i in range(10):\n",
        "        text = X_test[i]\n",
        "        true_label = le.inverse_transform([y_test[i]])[0]\n",
        "        idxs = test_dataset.text_to_indices(text)\n",
        "        xb = torch.tensor([idxs], dtype=torch.long).to(device)\n",
        "        cnn_model.to(device)\n",
        "        cnn_model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = cnn_model(xb)\n",
        "            pred_idx = int(torch.argmax(logits, dim=1).cpu().numpy()[0])\n",
        "        pred_label = le.inverse_transform([pred_idx])[0]\n",
        "        print(f\"Text (truncated): {text[:120]}...\")\n",
        "        print(f\"True: {true_label} | Pred: {pred_label}\")\n",
        "        print(\"-----\")\n",
        "\n",
        "    print(\"\\nAll tasks completed. Outputs:\")\n",
        "    print(\" - t-SNE plot saved as 'tsne_categories.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pbrx8SND5PLO"
      }
    }
  ]
}